{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "modelRes = YOLO(\"best.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "test model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ultralytics 8.3.18  Python-3.11.0 torch-2.5.0+cu124 CUDA:0 (NVIDIA GeForce RTX 3060, 12288MiB)\n",
      "YOLOv10s summary (fused): 293 layers, 8,037,282 parameters, 0 gradients, 24.5 GFLOPs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mScanning C:\\Users\\PC\\Desktop\\thesis\\detection\\data\\valid\\labels.cache... 486 images, 0 backgrounds, 0 corrupt: 100%|██████████| 486/486 [00:00<?, ?it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 31/31 [00:05<00:00,  6.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        486        791      0.846      0.695      0.789      0.547\n",
      "               Gernade        103        161      0.869      0.584      0.726       0.53\n",
      "                   Gun        204        313      0.923      0.771      0.886      0.618\n",
      "                 Knife        268        317      0.744      0.729      0.754      0.494\n",
      "Speed: 0.5ms preprocess, 6.6ms inference, 0.0ms loss, 0.2ms postprocess per image\n",
      "Results saved to \u001b[1mruns\\detect\\val\u001b[0m\n",
      "Model names:  {0: 'Gernade', 1: 'Gun', 2: 'Knife'}\n",
      "Class indices with average precision: [0 1 2]\n",
      "Average precision: [    0.52994     0.61777     0.49385]\n",
      "Average precision at IoU=0.50: [    0.72623     0.88637     0.75361]\n",
      "Class indices for average precision: [0 1 2]\n",
      "F1 score: [    0.69859     0.84054     0.73647]\n",
      "Mean average precision: 0.5471883298647278\n",
      "Mean average precision at IoU=0.50: 0.7887397781970646\n",
      "Mean average precision at IoU=0.75: 0.6366083364985254\n",
      "Mean average precision for different IoU thresholds: [    0.52994     0.61777     0.49385]\n",
      "Mean precision: 0.8457897684573896\n",
      "Mean recall: 0.6946069021780049\n",
      "Precision: [    0.86947     0.92349     0.74441]\n",
      "Recall: [    0.58385     0.77126     0.72871]\n"
     ]
    }
   ],
   "source": [
    "results = modelRes.val(data=\"data.yaml\")\n",
    "\n",
    "# Print specific metrics\n",
    "print(\"Model names: \", modelRes.names)\n",
    "print(\"Class indices with average precision:\", results.ap_class_index)\n",
    "print(\"Average precision:\", results.box.ap)\n",
    "print(\"Average precision at IoU=0.50:\", results.box.ap50)\n",
    "print(\"Class indices for average precision:\", results.box.ap_class_index)\n",
    "print(\"F1 score:\", results.box.f1)\n",
    "print(\"Mean average precision:\", results.box.map)\n",
    "print(\"Mean average precision at IoU=0.50:\", results.box.map50)\n",
    "print(\"Mean average precision at IoU=0.75:\", results.box.map75)\n",
    "print(\"Mean average precision for different IoU thresholds:\", results.box.maps)\n",
    "print(\"Mean precision:\", results.box.mp)\n",
    "print(\"Mean recall:\", results.box.mr)\n",
    "print(\"Precision:\", results.box.p)\n",
    "print(\"Recall:\", results.box.r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# Auto-configure CLAHE parameters based on the image\n",
    "def enhance_contrast_auto(gray_image):\n",
    "    # Get image dimensions\n",
    "    height, width = gray_image.shape\n",
    "\n",
    "    # Dynamically set the CLAHE parameters\n",
    "    # Clip limit adapts based on image intensity distribution\n",
    "    clip_limit = max(\n",
    "        2.0, min(4.0, gray_image.std() / 20)\n",
    "    )  # Adjusted based on image contrast\n",
    "    # Tile grid size adapts based on image size\n",
    "    grid_size = (\n",
    "        (8, 8) if max(height, width) <= 1000 else (16, 16)\n",
    "    )  # Larger for high-res\n",
    "\n",
    "    # Create CLAHE with dynamic parameters\n",
    "    clahe = cv2.createCLAHE(clipLimit=clip_limit, tileGridSize=grid_size)\n",
    "\n",
    "    # Apply CLAHE to enhance contrast\n",
    "    enhanced_image = clahe.apply(gray_image)\n",
    "    return enhanced_image\n",
    "\n",
    "\n",
    "def reduce_noise_auto(image):\n",
    "    \"\"\"\n",
    "    Applies a Bilateral Filter with adaptive parameters based on the input grayscale image.\n",
    "\n",
    "    :param image: Grayscale image (numpy array).\n",
    "    :return: Filtered image.\n",
    "    \"\"\"\n",
    "    # Calculate noise level (standard deviation)\n",
    "    noise_std = np.std(image)\n",
    "\n",
    "    # Image dimensions\n",
    "    height, width = image.shape\n",
    "    image_size = max(height, width)\n",
    "\n",
    "    # Adaptive parameters\n",
    "    d = max(5, image_size // 100)  # Neighborhood diameter, scaled by image size\n",
    "    sigma_color = noise_std * 10  # Adjust intensity influence based on noise level\n",
    "    sigma_space = d * 1.5  # Spatial influence, linked to diameter\n",
    "\n",
    "    # Apply Bilateral Filter\n",
    "    filtered_image = cv2.bilateralFilter(image, d, sigma_color, sigma_space)\n",
    "\n",
    "    return filtered_image\n",
    "\n",
    "\n",
    "def auto_edge_enhance(gray_image):\n",
    "    # Calculate the image contrast (standard deviation of pixel values)\n",
    "    contrast = gray_image.std()\n",
    "\n",
    "    # Set a base clip limit and tile grid size\n",
    "    clip_limit = 2.0  # default\n",
    "    tile_grid_size = (8, 8)  # default size\n",
    "\n",
    "    # Adjust clip limit based on image contrast (higher contrast = higher clip limit)\n",
    "    if contrast > 50:\n",
    "        clip_limit = 3.0\n",
    "    elif contrast > 30:\n",
    "        clip_limit = 2.5\n",
    "    else:\n",
    "        clip_limit = 2.0\n",
    "\n",
    "    # Adjust tile grid size based on image resolution (smaller images = smaller tiles)\n",
    "    height, width = gray_image.shape\n",
    "    if height < 500 or width < 500:\n",
    "        tile_grid_size = (4, 4)\n",
    "    elif height < 1000 or width < 1000:\n",
    "        tile_grid_size = (8, 8)\n",
    "    else:\n",
    "        tile_grid_size = (16, 16)\n",
    "\n",
    "    # Apply CLAHE with the auto-configured parameters\n",
    "    clahe = cv2.createCLAHE(clipLimit=clip_limit, tileGridSize=tile_grid_size)\n",
    "    clahe_img = clahe.apply(gray_image)\n",
    "\n",
    "    return clahe_img\n",
    "\n",
    "\n",
    "def dehaze_auto(gray_image):\n",
    "    # Calculate dynamic CLAHE parameters based on image properties\n",
    "    mean_brightness = gray_image.mean()\n",
    "    contrast_std = gray_image.std()\n",
    "\n",
    "    # Dynamically configure CLAHE\n",
    "    clip_limit = max(\n",
    "        2.0, min(4.0, contrast_std / 10)\n",
    "    )  # Adjust clip limit based on contrast\n",
    "    grid_size = (8, 8) if gray_image.shape[0] <= 1000 else (16, 16)  # Adjust grid size\n",
    "\n",
    "    # Apply CLAHE\n",
    "    clahe = cv2.createCLAHE(clipLimit=clip_limit, tileGridSize=grid_size)\n",
    "    enhanced_image = clahe.apply(gray_image)\n",
    "\n",
    "    # Normalize intensity if brightness is too low\n",
    "    if mean_brightness < 100:  # If the image is very dark\n",
    "        normalized_image = cv2.normalize(enhanced_image, None, 0, 255, cv2.NORM_MINMAX)\n",
    "        return normalized_image\n",
    "    else:\n",
    "        return enhanced_image\n",
    "\n",
    "\n",
    "def adaptive_unsharp_masking(gray_image):\n",
    "    \"\"\"\n",
    "    Automatically sharpens a grayscale image using unsharp masking with dynamic adjustments\n",
    "    based on image size, brightness, and contrast.\n",
    "\n",
    "    Args:\n",
    "        gray_image (numpy.ndarray): Input grayscale image (loaded with cv2).\n",
    "\n",
    "    Returns:\n",
    "        sharpened_image (numpy.ndarray): The sharpened grayscale image.\n",
    "    \"\"\"\n",
    "    # Get image dimensions\n",
    "    height, width = gray_image.shape\n",
    "\n",
    "    # Dynamically adjust blur kernel size based on image dimensions\n",
    "    # Larger images get larger kernels\n",
    "    kernel_size = (max(3, width // 500), max(3, height // 500))\n",
    "\n",
    "    # Apply Gaussian blur\n",
    "    blurred_image = cv2.GaussianBlur(gray_image, kernel_size, 0)\n",
    "\n",
    "    # Calculate the mean and standard deviation of the image\n",
    "    mean_intensity = np.mean(gray_image)\n",
    "    std_intensity = np.std(gray_image)\n",
    "\n",
    "    # Dynamically set alpha and beta\n",
    "    # Higher contrast (std_intensity) reduces alpha (to avoid over-sharpening)\n",
    "    # Lower brightness increases beta (to enhance edges more aggressively)\n",
    "    alpha = 1.0 + (std_intensity / 128)  # Base sharpness control\n",
    "    beta = -0.5 - (mean_intensity / 255)  # Edge control based on brightness\n",
    "\n",
    "    # Combine the original image and the blurred image for sharpening\n",
    "    sharpened_image = cv2.addWeighted(gray_image, alpha, blurred_image, beta, 0)\n",
    "\n",
    "    return sharpened_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 544x640 1 Knife, 134.2ms\n",
      "Speed: 8.0ms preprocess, 134.2ms inference, 50.4ms postprocess per image at shape (1, 3, 544, 640)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ultralytics.engine.results.Boxes object with attributes:\n",
       "\n",
       "cls: tensor([2.], device='cuda:0')\n",
       "conf: tensor([0.8434], device='cuda:0')\n",
       "data: tensor([[ 48.7240,  81.2650, 640.0000, 528.8535,   0.8434,   2.0000]], device='cuda:0')\n",
       "id: None\n",
       "is_track: False\n",
       "orig_shape: (536, 640)\n",
       "shape: torch.Size([1, 6])\n",
       "xywh: tensor([[344.3620, 305.0592, 591.2760, 447.5884]], device='cuda:0')\n",
       "xywhn: tensor([[0.5381, 0.5691, 0.9239, 0.8351]], device='cuda:0')\n",
       "xyxy: tensor([[ 48.7240,  81.2650, 640.0000, 528.8535]], device='cuda:0')\n",
       "xyxyn: tensor([[0.0761, 0.1516, 1.0000, 0.9867]], device='cuda:0')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# Read the PNG image\n",
    "png_image = cv2.imread('testKnife.jpg', cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "# resized_image = cv2.resize(png_image, (640, 640))\n",
    "denoised_image = reduce_noise_auto(png_image)\n",
    "enhance_contract_image = enhance_contrast_auto(denoised_image)\n",
    "unsharp_masking_image = adaptive_unsharp_masking(enhance_contract_image)\n",
    "\n",
    "input_image = cv2.merge([unsharp_masking_image, unsharp_masking_image, unsharp_masking_image])\n",
    "\n",
    "results = modelRes.predict(input_image,conf=0.30,)\n",
    "results[0].show()\n",
    "results[0].boxes"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
