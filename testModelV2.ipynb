{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "modelRes = YOLO(\"best.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "test model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ultralytics 8.3.18  Python-3.11.0 torch-2.5.0+cu124 CUDA:0 (NVIDIA GeForce RTX 3060, 12288MiB)\n",
      "YOLOv10s summary (fused): 293 layers, 8,037,282 parameters, 0 gradients, 24.5 GFLOPs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mScanning C:\\Users\\PC\\Desktop\\thesis\\detection\\data\\valid\\labels.cache... 486 images, 0 backgrounds, 0 corrupt: 100%|██████████| 486/486 [00:00<?, ?it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 31/31 [00:05<00:00,  6.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        486        791      0.846      0.695      0.789      0.547\n",
      "               Gernade        103        161      0.869      0.584      0.726       0.53\n",
      "                   Gun        204        313      0.923      0.771      0.886      0.618\n",
      "                 Knife        268        317      0.744      0.729      0.754      0.494\n",
      "Speed: 0.5ms preprocess, 6.6ms inference, 0.0ms loss, 0.2ms postprocess per image\n",
      "Results saved to \u001b[1mruns\\detect\\val\u001b[0m\n",
      "Model names:  {0: 'Gernade', 1: 'Gun', 2: 'Knife'}\n",
      "Class indices with average precision: [0 1 2]\n",
      "Average precision: [    0.52994     0.61777     0.49385]\n",
      "Average precision at IoU=0.50: [    0.72623     0.88637     0.75361]\n",
      "Class indices for average precision: [0 1 2]\n",
      "F1 score: [    0.69859     0.84054     0.73647]\n",
      "Mean average precision: 0.5471883298647278\n",
      "Mean average precision at IoU=0.50: 0.7887397781970646\n",
      "Mean average precision at IoU=0.75: 0.6366083364985254\n",
      "Mean average precision for different IoU thresholds: [    0.52994     0.61777     0.49385]\n",
      "Mean precision: 0.8457897684573896\n",
      "Mean recall: 0.6946069021780049\n",
      "Precision: [    0.86947     0.92349     0.74441]\n",
      "Recall: [    0.58385     0.77126     0.72871]\n"
     ]
    }
   ],
   "source": [
    "results = modelRes.val(data=\"data.yaml\")\n",
    "\n",
    "# Print specific metrics\n",
    "print(\"Model names: \", modelRes.names)\n",
    "print(\"Class indices with average precision:\", results.ap_class_index)\n",
    "print(\"Average precision:\", results.box.ap)\n",
    "print(\"Average precision at IoU=0.50:\", results.box.ap50)\n",
    "print(\"Class indices for average precision:\", results.box.ap_class_index)\n",
    "print(\"F1 score:\", results.box.f1)\n",
    "print(\"Mean average precision:\", results.box.map)\n",
    "print(\"Mean average precision at IoU=0.50:\", results.box.map50)\n",
    "print(\"Mean average precision at IoU=0.75:\", results.box.map75)\n",
    "print(\"Mean average precision for different IoU thresholds:\", results.box.maps)\n",
    "print(\"Mean precision:\", results.box.mp)\n",
    "print(\"Mean recall:\", results.box.mr)\n",
    "print(\"Precision:\", results.box.p)\n",
    "print(\"Recall:\", results.box.r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 640x640 1 Knife, 95.0ms\n",
      "Speed: 0.0ms preprocess, 95.0ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "def resize_with_aspect_ratio(image, width=None, height=None):\n",
    "    # Get the original image dimensions\n",
    "    h, w = image.shape[:2]\n",
    "\n",
    "    # Calculate the aspect ratio\n",
    "    aspect_ratio = w / h\n",
    "\n",
    "    if width is None:\n",
    "        # Calculate height based on the specified width\n",
    "        new_height = int(height / aspect_ratio)\n",
    "        resized_image = cv2.resize(image, (height, new_height))\n",
    "    else:\n",
    "        # Calculate width based on the specified height\n",
    "        new_width = int(width * aspect_ratio)\n",
    "        resized_image = cv2.resize(image, (new_width, width))\n",
    "\n",
    "    return resized_image\n",
    "\n",
    "\n",
    "# Read the PNG image\n",
    "png_image = cv2.imread('image.png')\n",
    "\n",
    "# Encode the image to memory as JPEG format (optional, only necessary if you need JPEG specifically)\n",
    "success, jpg_image = cv2.imencode('.jpg', png_image)\n",
    "\n",
    "# Convert it back to a NumPy array for YOLO input (if necessary)\n",
    "if success:\n",
    "    jpg_image_np = cv2.imdecode(np.frombuffer(jpg_image, np.uint8), cv2.IMREAD_COLOR)\n",
    "\n",
    "# Resize the image with an aspect ratio\n",
    "# resized_image = resize_with_aspect_ratio(jpg_image_np, width=640)\n",
    "\n",
    "# Convert image to RGB as YOLO models usually expect RGB input\n",
    "# input_image = cv2.cvtColor(resized_image, cv2.COLOR_RGB2GRAY)\n",
    "\n",
    "# Perform prediction using the YOLO model\n",
    "results = modelRes.predict(jpg_image_np,conf=0.10,)\n",
    "results[0].show()\n",
    "# results[0].boxes"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
